# simple_moe_pravite

> A minimal yet flexible **Mixture-of-Experts (MoE)** library built with [MLX](https://github.com/ml-explore/mlx) for research and experimentation.  
> Designed for clarity, modularity, and easy extension â€” perfect for learning, prototyping, and building your own gating networks.

---

## ğŸš€ Features

- **Lightweight & Modular** â€” each component (experts, gate, loss, MoE layer) is self-contained.  
- **Soft / Top-K Gating** â€” interchangeable gating strategies via `gate_factory`.  
- **Load Balancing Loss** â€” optional auxiliary regularization to improve expert utilization.  
- **Temperature Control** â€” stable `log_softmax` and temperature-aware gates.  
- **Task Heads** â€” built-in classification and regression heads.  
- **Clean API** â€” build a custom MoE model in just a few lines.

---

## ğŸ§© Installation

### 1ï¸âƒ£ Clone or install from GitHub
```bash
git clone https://github.com/<yourname>/SimpleMoEs.git
cd SimpleMoEs
pip install -e .
```

## ä¸ªäººä½¿ç”¨çš„MoEç¤ºä¾‹
